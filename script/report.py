"""
Benchmark Report Generator.

This script aggregates the distributed CSV results produced by the benchmark
suite, combines them with memory usage data collected by Memray, and generates
a comprehensive report in either HTML or Markdown format.
"""

import os
import re
import csv
import sys
import json
import pickle
import pathlib
import argparse

from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Any, Tuple


def format_bytes(size: float) -> str:
    """
    Convert a raw byte count into a human-readable string.

    Args:
        size: The size in bytes.

    Returns:
        A string formatted with the appropriate unit (B, KiB, MiB, etc.).
    """
    for unit in ["B", "KiB", "MiB", "GiB"]:
        if size < 1024.0:
            return f"{size:.2f} {unit}"
        size /= 1024.0
    return f"{size:.2f} TiB"


def get_memory_map(results_dir: pathlib.Path) -> Dict[str, int]:
    """
    Scan memray metadata files to map test IDs to peak memory usage.

    Memray produces one metadata file per process/test run. This function
    opens each pickled metadata file, extracts the test ID, and retrieves
    the peak memory usage statistic.

    Args:
        results_dir: The root directory containing the 'memray' folder.

    Returns:
        A dictionary mapping the test ID string (e.g. 'dpll-cnf-basic')
        to the peak memory usage in bytes.
    """
    memray_meta_dir = results_dir / "memray" / "metadata"
    memory_map: Dict[str, int] = {}

    if not memray_meta_dir.exists():
        return memory_map

    print(f"scanning memory logs in {memray_meta_dir}...")

    # glob for all metadata files. memray names them with a timestamp/uuid.
    for meta_file in memray_meta_dir.glob("memory-*.metadata"):
        try:
            with open(meta_file, "rb") as f:
                # memray metadata is a pickled python object. we load it
                # to access the .test_id and .metadata.peak_memory attributes.
                data = pickle.load(f)

                # the test_id stored by memray is the full pytest node id,
                # which looks like: 'suites/benchmark.py::test[dpll-cnf-basic]'.
                # we need to extract the part inside the brackets to match
                # the ID we stored in our csv files.
                match = re.search(r"\[(.*?)\]", data.test_id)
                if match:
                    test_id_key = match.group(1)
                    memory_map[test_id_key] = data.metadata.peak_memory
        except Exception:
            # if a file is incomplete or corrupted (e.g. forced process kill),
            # we skip it to ensure the report generation finishes for the rest.
            pass

    return memory_map


def load_data(result_root: str) -> Dict[Tuple[str, str], List[Dict[str, Any]]]:
    """
    Load and aggregate all CSV fragments from the results directory.

    This function performs the join operation between the runtime metrics
    (CSV files) and the memory metrics (Memray files).

    Args:
        result_root: Path string to the 'result' directory.

    Returns:
        A dictionary where the key is a tuple of (problem_type, problem_name)
        and the value is a list of result dictionaries belonging to that group.
    """
    root_path = pathlib.Path(result_root)
    bench_dir = root_path / "benchs"

    if not bench_dir.exists():
        print(f"error: directory not found: {bench_dir}")
        sys.exit(1)

    # 1. build the memory map first so we can look up values as we read csvs.
    memory_map = get_memory_map(root_path)

    # 2. find all csv files generated by the distributed test runners.
    csv_files = list(bench_dir.glob("*.csv"))
    data = defaultdict(list)

    if not csv_files:
        print("no benchmark results found.")
        sys.exit(0)

    print(f"aggregating {len(csv_files)} result files...")

    for csv_path in csv_files:
        try:
            with open(csv_path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # convert numerical strings back to proper types for sorting.
                    row["wall_time"] = float(row["wall_time_sec"])
                    row["cpu_time"] = float(row["cpu_time_sec"])
                    row["verified"] = row["verified"] == "True"

                    # parse the stats json blob back into a python dictionary.
                    try:
                        row["stats"] = json.loads(row["stats"])
                    except (json.JSONDecodeError, TypeError):
                        row["stats"] = {}

                    # retrieve memory usage using the 'id' column as the key.
                    test_id = row.get("id", "")
                    row["memory"] = memory_map.get(test_id, 0)

                    # group results by problem so we can compare solvers
                    # side-by-side for the same input.
                    key = (row["type"], row["problem"])
                    data[key].append(row)
        except Exception as e:
            print(f"warning: failed to read {csv_path.name}: {e}")

    return data


def generate_markdown(
    grouped_data: Dict[Tuple[str, str], List[Dict[str, Any]]], output_path: str
) -> None:
    """
    Generate a simple Markdown report table.

    Args:
        grouped_data: The aggregated benchmark data.
        output_path: Where to save the .md file.
    """
    lines = []
    lines.append("# Benchmark Report")
    lines.append(f"> generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # sort by problem type, then problem name for a logical flow.
    sorted_keys = sorted(grouped_data.keys(), key=lambda x: (x[0], x[1]))

    for p_type, p_name in sorted_keys:
        rows = grouped_data[(p_type, p_name)]
        if not rows:
            continue

        # calculate the fastest time among verified results to highlight the winner.
        # if no results are verified, we default to the absolute min time.
        verified_rows = [r for r in rows if r["verified"]]
        if verified_rows:
            min_time = min(r["wall_time"] for r in verified_rows)
        else:
            min_time = min(r["wall_time"] for r in rows)

        lines.append(f"### ({p_type}) _{p_name}_")
        lines.append("| solver | status | ver | wall (s) | cpu (s) | memory | stats |")
        lines.append("|:---|:---|:---:|---:|---:|---:|:---|")

        # sort rows primarily by verification status (verified first),
        # and secondarily by wall time (fastest first).
        # in python sort tuples: False < True (0 < 1).
        # we want verified (True) first, so we negate verification:
        # (not verified) -> False (0) comes before True (1).
        rows.sort(key=lambda x: (not x["verified"], x["wall_time"]))

        for r in rows:
            # only highlight the solver if it is the fastest AND verified.
            is_fastest = r["wall_time"] == min_time and r["verified"] and len(rows) > 1
            solver = f"**{r['solver']}**" if is_fastest else r["solver"]

            # use emojis for verification status in markdown.
            ver_icon = "ðŸŸ¢" if r["verified"] else "ðŸ”´"
            mem_str = format_bytes(r["memory"])

            # format the stats dictionary into a compact key=value string.
            stats_str = ", ".join(f"{k}={v}" for k, v in r["stats"].items())

            lines.append(
                f"| {solver} | {r['status']} | {ver_icon} | "
                f"{r['wall_time']:.4f} | {r['cpu_time']:.4f} | "
                f"{mem_str} | {stats_str} |"
            )

        lines.append("")

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print(f"report written to {output_path}")


def generate_html(
    grouped_data: Dict[Tuple[str, str], List[Dict[str, Any]]], output_path: str
) -> None:
    """
    Generate a styled HTML report with progress bars.

    Args:
        grouped_data: The aggregated benchmark data.
        output_path: Where to save the .html file.
    """
    # use a multi-line string to define the html skeleton and css styles.
    # we use f-string syntax to inject the current timestamp.
    html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Solver Benchmark</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {{
            --bg: #f8fafc;
            --card: #ffffff;
            --text: #0f172a;
            --muted: #64748b;
            --border: #e2e8f0;
            --primary: #10b981;
            --primary-bg: #ecfdf5;
            --danger: #ef4444;
            --danger-bg: #fef2f2;
        }}

        body {{
            font-family: 'Inter', sans-serif;
            background: var(--bg);
            color: var(--text);
            padding: 40px 20px;
            margin: 0;
            line-height: 1.5;
        }}

        .container {{
            max_width: 1200px;
            margin: 0 auto;
        }}

        header {{
            margin-bottom: 40px;
        }}

        h1 {{
            margin: 0 0 8px 0;
            font-size: 2rem;
            letter-spacing: -0.025em;
        }}

        .meta {{
            color: var(--muted);
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.875rem;
        }}

        .problem-card {{
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 12px;
            margin-bottom: 32px;
            overflow: hidden;
            box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
        }}

        .card-header {{
            padding: 16px 24px;
            border-bottom: 1px solid var(--border);
            background: #ffffff;
            display: flex;
            align-items: center;
            gap: 12px;
        }}

        .badge {{
            background: var(--bg);
            border: 1px solid var(--border);
            padding: 4px 10px;
            border-radius: 9999px;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            color: var(--muted);
            letter-spacing: 0.05em;
        }}

        .p-name {{
            font-weight: 600;
            font-family: 'JetBrains Mono', monospace;
            font-size: 1rem;
        }}

        .table-container {{
            overflow-x: auto;
        }}

        table {{
            width: 100%;
            border-collapse: collapse;
            font-size: 0.875rem;
            min-width: 800px;
        }}

        th {{
            text-align: left;
            padding: 12px 24px;
            background: #f8fafc;
            font-weight: 600;
            color: var(--muted);
            text-transform: uppercase;
            font-size: 0.75rem;
            letter-spacing: 0.05em;
            border-bottom: 1px solid var(--border);
        }}

        td {{
            padding: 16px 24px;
            border-bottom: 1px solid var(--border);
            vertical-align: middle;
        }}

        tr:last-child td {{
            border-bottom: none;
        }}

        /* row styling */
        .row-fastest {{
            background-color: var(--primary-bg);
        }}
        .row-fastest td {{
            color: #047857;
        }}
        .row-fastest .solver {{
            font-weight: 700;
        }}
        .row-unverified {{
            background-color: #fafafa;
            color: var(--muted);
        }}

        /* progress bar */
        .bar-wrapper {{
            height: 6px;
            background: #e2e8f0;
            width: 100%;
            margin-top: 8px;
            border-radius: 9999px;
            overflow: hidden;
        }}
        .bar-fill {{
            height: 100%;
            background: #cbd5e1;
            border-radius: 9999px;
            transition: width 0.5s ease;
        }}
        .row-fastest .bar-fill {{
            background: var(--primary);
        }}
        .row-unverified .bar-fill {{
            background: #e2e8f0;
        }}

        .mono {{
            font-family: 'JetBrains Mono', monospace;
        }}

        .stats {{
            color: var(--muted);
            font-size: 0.75rem;
            line-height: 1.4;
        }}

        .status-pill {{
            display: inline-flex;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 500;
            font-size: 0.75rem;
            background: #f1f5f9;
        }}

        /* verification dots */
        .dot {{
            height: 10px;
            width: 10px;
            border-radius: 50%;
            display: inline-block;
        }}
        .dot-green {{
            background-color: var(--primary);
            box-shadow: 0 0 0 2px var(--primary-bg);
        }}
        .dot-red {{
            background-color: var(--danger);
            box-shadow: 0 0 0 2px var(--danger-bg);
        }}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Benchmark Report</h1>
            <div class="meta">Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</div>
        </header>
    """

    sorted_keys = sorted(grouped_data.keys(), key=lambda x: (x[0], x[1]))

    for p_type, p_name in sorted_keys:
        rows = grouped_data[(p_type, p_name)]
        if not rows:
            continue

        # calculate the fastest time among verified results to highlight the winner.
        verified_rows = [r for r in rows if r["verified"]]
        if verified_rows:
            min_time = min(r["wall_time"] for r in verified_rows)
        else:
            min_time = min(r["wall_time"] for r in rows)

        html_content += f"""
        <div class="problem-card">
            <div class="card-header">
                <span class="badge">{p_type}</span>
                <span class="p-name">{p_name}</span>
            </div>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th width="15%">Solver</th>
                            <th width="8%">Status</th>
                            <th width="5%" style="text-align: center;">Ver</th>
                            <th width="20%">Wall Time</th>
                            <th width="10%">CPU Time</th>
                            <th width="10%">Memory</th>
                            <th>Stats</th>
                        </tr>
                    </thead>
                    <tbody>
        """

        # sort rows primarily by verification status (verified first),
        # and secondarily by wall time (fastest first).
        rows.sort(key=lambda x: (not x["verified"], x["wall_time"]))

        for r in rows:
            is_verified = r["verified"]

            # determine highlighting. we only highlight verified winners.
            is_fastest = r["wall_time"] == min_time and is_verified and len(rows) > 1

            # determine css class for the row.
            row_class = ""
            if is_fastest:
                row_class = "row-fastest"
            elif not is_verified:
                row_class = "row-unverified"

            # calculate percentage width for the timing bar.
            max_time = max(row["wall_time"] for row in rows)
            pct = (r["wall_time"] / max_time * 100) if max_time > 0 else 0

            # format stats and verification dot.
            stats_html = ", ".join(f"{k}:{v}" for k, v in r["stats"].items())

            dot_class = "dot-green" if is_verified else "dot-red"
            ver_html = f'<span class="dot {dot_class}"></span>'

            html_content += f"""
                        <tr class="{row_class}">
                            <td class="solver">{r["solver"]}</td>
                            <td><span class="status-pill">{r["status"]}</span></td>
                            <td style="text-align: center;">{ver_html}</td>
                            <td>
                                <div class="mono">{r["wall_time"]:.4f}s</div>
                                <div class="bar-wrapper">
                                    <div class="bar-fill" style="width:{pct}%"></div>
                                </div>
                            </td>
                            <td class="mono">{r["cpu_time"]:.4f}s</td>
                            <td class="mono">{format_bytes(r["memory"])}</td>
                            <td class="stats">{stats_html}</td>
                        </tr>
            """

        html_content += """
                    </tbody>
                </table>
            </div>
        </div>
        """

    html_content += """
    </div>
</body>
</html>
    """

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    print(f"report written to {output_path}")


def main():
    """
    Main entry point for the report script.

    Parses arguments, orchestrates the data loading, and triggers the
    appropriate generation function.
    """
    parser = argparse.ArgumentParser()

    # allow the user to specify where the 'result' directory is, defaulting
    # to the standard location 'result' in the current working directory.
    parser.add_argument(
        "output",
        nargs="?",
        default="result",
        help="path to the result directory (default: result)",
    )

    parser.add_argument(
        "-f",
        "--format",
        choices=["html", "markdown"],
        default="html",
        help="output format (default: html)",
    )

    args = parser.parse_args()

    # load and aggregate the distributed results.
    data = load_data(args.output)

    # define the output filename based on the input directory name.
    output_base = os.path.join(args.output, "benchmark")

    if args.format == "html":
        generate_html(data, output_base + ".html")
    else:
        generate_markdown(data, output_base + ".md")


if __name__ == "__main__":
    main()
